{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzXSKQ_TA18R",
        "outputId": "81c9eadb-2e45-49b7-ccb3-a703da3fdd9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting influxdb_client\n",
            "  Downloading influxdb_client-1.49.0-py3-none-any.whl.metadata (65 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting reactivex>=4.0.4 (from influxdb_client)\n",
            "  Downloading reactivex-4.0.4-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from influxdb_client) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from influxdb_client) (2.9.0.post0)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from influxdb_client) (75.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from influxdb_client) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->influxdb_client) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from reactivex>=4.0.4->influxdb_client) (4.14.0)\n",
            "Downloading influxdb_client-1.49.0-py3-none-any.whl (746 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.3/746.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading reactivex-4.0.4-py3-none-any.whl (217 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.8/217.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reactivex, influxdb_client\n",
            "Successfully installed influxdb_client-1.49.0 reactivex-4.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install influxdb_client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ydata_profiling # Optional for user. It generates HTML based report of the data."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1j5bGF5Pl0V",
        "outputId": "b54f2207-495f-453a-d086-4821e841f224"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ydata_profiling\n",
            "  Downloading ydata_profiling-4.16.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: scipy<1.16,>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (1.15.3)\n",
            "Requirement already satisfied: pandas!=1.4.0,<3.0,>1.1 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (2.2.2)\n",
            "Requirement already satisfied: matplotlib<=3.10,>=3.5 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (3.10.0)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (2.11.5)\n",
            "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (6.0.2)\n",
            "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (3.1.6)\n",
            "Collecting visions<0.8.2,>=0.7.5 (from visions[type_image_path]<0.8.2,>=0.7.5->ydata_profiling)\n",
            "  Downloading visions-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.2,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (2.0.2)\n",
            "Collecting htmlmin==0.1.12 (from ydata_profiling)\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting phik<0.13,>=0.11.1 (from ydata_profiling)\n",
            "  Downloading phik-0.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2.24.0 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (2.32.3)\n",
            "Requirement already satisfied: tqdm<5,>=4.48.2 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (4.67.1)\n",
            "Requirement already satisfied: seaborn<0.14,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (0.13.2)\n",
            "Collecting multimethod<2,>=1.4 (from ydata_profiling)\n",
            "  Downloading multimethod-1.12-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: statsmodels<1,>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (0.14.4)\n",
            "Requirement already satisfied: typeguard<5,>=3 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (4.4.2)\n",
            "Collecting imagehash==4.3.1 (from ydata_profiling)\n",
            "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: wordcloud>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (1.9.4)\n",
            "Collecting dacite>=1.8 (from ydata_profiling)\n",
            "  Downloading dacite-1.9.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numba<=0.61,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from ydata_profiling) (0.60.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (from imagehash==4.3.1->ydata_profiling) (1.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from imagehash==4.3.1->ydata_profiling) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<3.2,>=2.11.1->ydata_profiling) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10,>=3.5->ydata_profiling) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10,>=3.5->ydata_profiling) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10,>=3.5->ydata_profiling) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10,>=3.5->ydata_profiling) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10,>=3.5->ydata_profiling) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10,>=3.5->ydata_profiling) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10,>=3.5->ydata_profiling) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba<=0.61,>=0.56.0->ydata_profiling) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=1.4.0,<3.0,>1.1->ydata_profiling) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=1.4.0,<3.0,>1.1->ydata_profiling) (2025.2)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from phik<0.13,>=0.11.1->ydata_profiling) (1.5.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ydata_profiling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ydata_profiling) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ydata_profiling) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ydata_profiling) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.24.0->ydata_profiling) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.24.0->ydata_profiling) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.24.0->ydata_profiling) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.24.0->ydata_profiling) (2025.4.26)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels<1,>=0.13.2->ydata_profiling) (1.0.1)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.11/dist-packages (from visions<0.8.2,>=0.7.5->visions[type_image_path]<0.8.2,>=0.7.5->ydata_profiling) (25.3.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.11/dist-packages (from visions<0.8.2,>=0.7.5->visions[type_image_path]<0.8.2,>=0.7.5->ydata_profiling) (3.5)\n",
            "Collecting puremagic (from visions<0.8.2,>=0.7.5->visions[type_image_path]<0.8.2,>=0.7.5->ydata_profiling)\n",
            "  Downloading puremagic-1.29-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<=3.10,>=3.5->ydata_profiling) (1.17.0)\n",
            "Downloading ydata_profiling-4.16.1-py2.py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.1/400.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dacite-1.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading multimethod-1.12-py3-none-any.whl (10 kB)\n",
            "Downloading phik-0.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (687 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m687.8/687.8 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading visions-0.8.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading puremagic-1.29-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: htmlmin\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27081 sha256=5b88058293052e86676e13d4f7c1b8ee6b99b87388e5ed185afd80c70c5ee0fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/55/1a/19cd535375ed1ede0c996405ebffe34b196d78e2d9545723a2\n",
            "Successfully built htmlmin\n",
            "Installing collected packages: puremagic, htmlmin, multimethod, dacite, imagehash, visions, phik, ydata_profiling\n",
            "Successfully installed dacite-1.9.2 htmlmin-0.1.12 imagehash-4.3.1 multimethod-1.12 phik-0.12.4 puremagic-1.29 visions-0.8.1 ydata_profiling-4.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from influxdb_client import InfluxDBClient\n",
        "import warnings\n",
        "#from ydata_profiling import ProfileReport # Keep if user wants to generate reports\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "aDqH-eeTPPkf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define InfluxDB connection parameters\n",
        "url = \"http://kammeyer.uk:8086\"\n",
        "token = \"CPSJ6xw1U72IcJjfLgzaukP24o1CL3grIQuvaw-Zq1MK9htUYNPwFUdKEalwl2-xMHFrVKOgG8tRFLgIkoneBw==\"\n",
        "org = \"591d9e9c3fc5e3ee\""
      ],
      "metadata": {
        "id": "dJymJaPbA_ws"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Establish InfluxDB Connection ---\n",
        "try:\n",
        "    client = InfluxDBClient(url=url, token=token, org=org)\n",
        "    print(\"✅ Successfully connected to InfluxDB\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error Connecting to InfluxDB: {e}\")\n",
        "    # It's better to raise an error or exit here if connection is critical\n",
        "    raise\n",
        "\n",
        "query_api = client.query_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uj7OBIeBGq_",
        "outputId": "9c2ab0f8-1273-4fc3-c28e-979efa795f7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully connected to InfluxDB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Function: Query and Pivot Data ---\n",
        "def fetch_pivoted_data(bucket, start=\"-6mo\"):\n",
        "    \"\"\"\n",
        "    Fetches data from InfluxDB and pivots it into a DataFrame.\n",
        "    Returns a list of DataFrames as query_data_frame can return multiple tables.\n",
        "    \"\"\"\n",
        "    query = f'''\n",
        "    from(bucket:\"{bucket}\")\n",
        "    |> range(start: {start})\n",
        "    |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
        "    '''\n",
        "    try:\n",
        "        df = query_api.query_data_frame(query)\n",
        "        if isinstance(df, list) and len(df) > 0:\n",
        "            # Ensure _time is datetime and sort for all returned DFs\n",
        "            for d in df:\n",
        "                if '_time' in d.columns:\n",
        "                    d['_time'] = pd.to_datetime(d['_time']).dt.floor('min') # Keep flooring for initial precision\n",
        "                    d.sort_values('_time', inplace=True)\n",
        "            return df\n",
        "        elif not isinstance(df, list) and '_time' in df.columns:\n",
        "            # Handle case where a single DataFrame is returned directly\n",
        "            df['_time'] = pd.to_datetime(df['_time']).dt.floor('min') # Keep flooring for initial precision\n",
        "            df.sort_values('_time', inplace=True)\n",
        "            return [df]\n",
        "        else:\n",
        "            print(f\"❌ No data or unexpected format from bucket '{bucket}'.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error querying bucket '{bucket}': {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "6E8xciA5BKMs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Data ---\n",
        "# Fetch CO2 data. Based on Carbon_Intensity_Forecast_Success.ipynb, 'carbonIntensity' and\n",
        "# 'fossilFuelPercentage' are expected in the first DataFrame returned from the 'co2' bucket.\n",
        "co2_dfs = fetch_pivoted_data(\"co2\")\n",
        "if not co2_dfs or len(co2_dfs) == 0:\n",
        "    print(\"❌ CO₂ data incomplete or not available.\")\n",
        "    raise ValueError(\"CO₂ data is essential and not available.\")\n",
        "co2_data_df = co2_dfs[0] # Assuming this DataFrame contains 'carbonIntensity' and 'fossilFuelPercentage'\n",
        "\n",
        "energy_df_list = fetch_pivoted_data(\"energy\")\n",
        "price_df_list = fetch_pivoted_data(\"price\")\n",
        "\n",
        "if not energy_df_list or energy_df_list[0].empty:\n",
        "    print(\"⚠️ Energy data not available. Exiting.\")\n",
        "    raise ValueError(\"Energy data is essential and not available.\")\n",
        "if not price_df_list or price_df_list[0].empty:\n",
        "    print(\"⚠️ Price data not available. Exiting.\")\n",
        "    raise ValueError(\"Price data is essential and not available.\")\n",
        "\n",
        "energy_df = energy_df_list[0]\n",
        "price_df = price_df_list[0]"
      ],
      "metadata": {
        "id": "SSSBe6kbBNNH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Preprocessing Function ---\n",
        "def preprocess_energy_data_for_foundation_model(energy_df, price_df, co2_data_df):\n",
        "    \"\"\"\n",
        "    Preprocess energy data for time series foundation models like LagLlama.\n",
        "\n",
        "    Args:\n",
        "        energy_df: Pandas DataFrame with energy production data.\n",
        "        price_df: Pandas DataFrame with price data.\n",
        "        co2_data_df: Pandas DataFrame with CO2 metrics (e.g., 'carbonIntensity', 'fossilFuelPercentage').\n",
        "\n",
        "    Returns:\n",
        "        dict: Contains processed datasets and metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Step 1: Data Cleaning and Preparation\")\n",
        "    # Merge energy and price data\n",
        "    merged_energy_price = pd.merge_asof(\n",
        "        energy_df, price_df, on='_time', direction='nearest', tolerance=pd.Timedelta('2min')\n",
        "    ).dropna(subset=['_time'])\n",
        "    merged_energy_price.ffill(inplace=True) # Use ffill() instead of fillna(method='ffill')\n",
        "\n",
        "    required_co2_cols = []\n",
        "    if 'carbonIntensity' in co2_data_df.columns:\n",
        "        required_co2_cols.append('carbonIntensity')\n",
        "    else:\n",
        "        raise ValueError(\"Column 'carbonIntensity' not found in CO2 data. This is a required target.\")\n",
        "\n",
        "    if 'fossilFuelPercentage' in co2_data_df.columns:\n",
        "        required_co2_cols.append('fossilFuelPercentage')\n",
        "    else:\n",
        "        print(\"❗ Warning: 'fossilFuelPercentage' not found in CO2 data. Renewable percentage will be calculated, not derived.\")\n",
        "\n",
        "    # Merge the combined energy_price data with the CO2 data\n",
        "    # Use 'inner' merge to ensure only matching timestamps are kept\n",
        "    # df_raw_merged will now contain all columns from merged_energy_price + required_co2_cols\n",
        "    df_raw_merged = pd.merge(merged_energy_price, co2_data_df[['_time'] + required_co2_cols],\n",
        "                            on='_time', how='inner').set_index('_time')\n",
        "    df_raw_merged.index.name = None\n",
        "\n",
        "\n",
        "    # Define energy sources and ensure all expected columns are present\n",
        "    energy_sources = [\n",
        "        'Biomasse', 'Braunkohle', 'Erdgas', 'Photovoltaik', 'Pumpspeicher',\n",
        "        'Sonstige Erneuerbare', 'Sonstige Konventionelle', 'Steinkohle',\n",
        "        'Wasserkraft', 'Wind Offshore', 'Wind Onshore'\n",
        "    ]\n",
        "    # Ensure all energy_sources are numerical and available in df_raw_merged\n",
        "    energy_sources = [col for col in energy_sources if col in df_raw_merged.columns]\n",
        "\n",
        "    # Define the primary target columns as per meeting remarks (price and carbonIntensity)\n",
        "    primary_target_columns = ['price.day_ahead_auction.price', 'carbonIntensity']\n",
        "    # Ensure primary_target_columns are available in df_raw_merged\n",
        "    primary_target_columns = [col for col in primary_target_columns if col in df_raw_merged.columns]\n",
        "\n",
        "\n",
        "    # Identify all features that will be used by LagLlama (targets + dynamic real features)\n",
        "    # This list should define the *final set of columns* we want to have *before* resampling.\n",
        "    all_numerical_features_to_keep = list(set(energy_sources + primary_target_columns + required_co2_cols)) # required_co2_cols includes fossilFuelPercentage if present\n",
        "\n",
        "\n",
        "    # Filter df_raw_merged to only include these intended numerical features\n",
        "    # Ensure numerical conversion for these features before proceeding, especially for price\n",
        "    df_raw_merged_numeric = df_raw_merged.copy() # Create a copy to avoid SettingWithCopyWarning\n",
        "    for col in all_numerical_features_to_keep:\n",
        "        if col in df_raw_merged_numeric.columns:\n",
        "            # Convert to numeric, coercing errors (non-numeric values become NaN)\n",
        "            df_raw_merged_numeric[col] = pd.to_numeric(df_raw_merged_numeric[col], errors='coerce')\n",
        "        else:\n",
        "            print(f\"❗ Warning: Expected numerical feature '{col}' not found in df_raw_merged. It will be excluded from the model input.\")\n",
        "\n",
        "    # Drop columns that are not numerical after conversion, or are no longer in our `all_numerical_features_to_keep` list\n",
        "    # Use select_dtypes again after potential conversions to ensure only numbers are kept.\n",
        "    df_raw_merged_numeric = df_raw_merged_numeric[\n",
        "        [col for col in all_numerical_features_to_keep if col in df_raw_merged_numeric.columns]\n",
        "    ].select_dtypes(include=np.number)\n",
        "\n",
        "\n",
        "    # --- Resample to hourly data as per meeting remarks (In [7]) ---\n",
        "    # Apply resampling to the DataFrame containing only numerical columns\n",
        "    df_processed = df_raw_merged_numeric.resample('H').mean()\n",
        "    print(\"Data successfully resampled to hourly mean data.\")\n",
        "\n",
        "    print(f\"Data shape after cleaning, merging and hourly resampling: {df_processed.shape}\")\n",
        "    print(f\"Date range: {df_processed.index.min()} to {df_processed.index.max()}\")\n",
        "    print(f\"Columns after selection for model and resampling: {list(df_processed.columns)}\")\n",
        "\n",
        "    # 2. Handle Missing Values (post-resampling)\n",
        "    print(\"\\nStep 2: Handling Missing Values (post-resampling)\")\n",
        "    missing_before_post_resample = df_processed.isnull().sum().sum()\n",
        "\n",
        "    for col in df_processed.columns:\n",
        "        df_processed[col] = df_processed[col].ffill().bfill()\n",
        "\n",
        "    missing_after_post_resample = df_processed.isnull().sum().sum()\n",
        "    print(f\"Missing values - Before: {missing_before_post_resample}, After: {missing_after_post_resample}\")\n",
        "\n",
        "    if df_processed.isnull().sum().sum() > 0:\n",
        "        print(\"❗ Warning: Some NaN values still exist after ffill/bfill. Filling with 0 as a last resort.\")\n",
        "        df_processed.fillna(0, inplace=True)\n",
        "\n",
        "    # 3. Create Time-based Features\n",
        "    print(\"\\nStep 3: Creating Time-based Features\")\n",
        "    df_processed['year'] = df_processed.index.year\n",
        "    df_processed['month'] = df_processed.index.month\n",
        "    df_processed['day'] = df_processed.index.day\n",
        "    df_processed['hour'] = df_processed.index.hour\n",
        "    df_processed['dayofweek'] = df_processed.index.dayofweek\n",
        "    df_processed['dayofyear'] = df_processed.index.dayofyear\n",
        "    df_processed['quarter'] = df_processed.index.quarter\n",
        "\n",
        "    # Cyclical encoding for time features (important for ML models)\n",
        "    df_processed['month_sin'] = np.sin(2 * np.pi * df_processed['month'] / 12)\n",
        "    df_processed['month_cos'] = np.cos(2 * np.pi * df_processed['month'] / 12)\n",
        "    df_processed['hour_sin'] = np.sin(2 * np.pi * df_processed['hour'] / 24)\n",
        "    df_processed['hour_cos'] = np.cos(2 * np.pi * df_processed['hour'] / 24)\n",
        "    df_processed['dayofweek_sin'] = np.sin(2 * np.pi * df_processed['dayofweek'] / 7)\n",
        "    df_processed['dayofweek_cos'] = np.cos(2 * np.pi * df_processed['dayofweek'] / 7)\n",
        "\n",
        "\n",
        "    # 4. Handle Renewable Percentage and CO2 Intensity as per meeting remarks (In [11])\n",
        "    print(\"\\nStep 4: Handling Renewable Percentage and CO2 Intensity\")\n",
        "\n",
        "    # If 'fossilFuelPercentage' is available, derive 'renewable_percentage' directly\n",
        "    if 'fossilFuelPercentage' in df_processed.columns: # Check if fossilFuelPercentage is in the already processed df\n",
        "        df_processed['renewable_percentage'] = 100 - df_processed['fossilFuelPercentage']\n",
        "        print(\"Using 'fossilFuelPercentage' to derive 'renewable_percentage'.\")\n",
        "    else:\n",
        "        # Calculate if 'fossilFuelPercentage' not available (original method, less preferred now)\n",
        "        # Total renewable energy (needed for calculation if not derived)\n",
        "        renewable_sources_calc = [\n",
        "            'Biomasse', 'Photovoltaik', 'Sonstige Erneuerbare',\n",
        "            'Wasserkraft', 'Wind Offshore', 'Wind Onshore'\n",
        "        ]\n",
        "        renewable_sources_calc = [col for col in renewable_sources_calc if col in df_processed.columns]\n",
        "        df_processed['total_renewable'] = df_processed[renewable_sources_calc].sum(axis=1)\n",
        "\n",
        "        conventional_sources = ['Braunkohle', 'Erdgas', 'Sonstige Konventionelle', 'Steinkohle']\n",
        "        conventional_sources = [col for col in conventional_sources if col in df_processed.columns]\n",
        "        df_processed['total_conventional'] = df_processed[conventional_sources].sum(axis=1)\n",
        "\n",
        "        df_processed['total_energy'] = df_processed['total_renewable'] + df_processed['total_conventional']\n",
        "        if 'Pumpspeicher' in df_processed.columns:\n",
        "            df_processed['total_energy'] += df_processed['Pumpspeicher'] # Add Pumpspeicher if available\n",
        "\n",
        "        df_processed['renewable_percentage'] = (df_processed['total_renewable'] / df_processed['total_energy']) * 100\n",
        "        df_processed['renewable_percentage'].fillna(0, inplace=True)\n",
        "        print(\"Calculating 'renewable_percentage' as 'fossilFuelPercentage' not found.\")\n",
        "\n",
        "    # 'co2_intensity' is explicitly stated to be equivalent to 'carbonIntensity' (In [11] remarks)\n",
        "    if 'carbonIntensity' in df_processed.columns:\n",
        "        df_processed['co2_intensity'] = df_processed['carbonIntensity']\n",
        "        print(\"Using 'carbonIntensity' directly for 'co2_intensity' as per remarks.\")\n",
        "    else:\n",
        "        # Fallback calculation if carbonIntensity is missing (unlikely if required as target)\n",
        "        print(\"❗ Error: 'carbonIntensity' not in processed DataFrame for co2_intensity calculation. This should not happen if it's a required target.\")\n",
        "        # If 'carbonIntensity' is truly missing and not in df_processed, then 'co2_intensity' won't be created.\n",
        "        # This branch indicates a deeper data availability issue from the InfluxDB fetch or earlier merges.\n",
        "\n",
        "\n",
        "    # 5. Create Different Dataset Formats for Foundation Models\n",
        "    print(\"\\nStep 5: Creating Foundation Model Ready Formats\")\n",
        "\n",
        "    # model_input_features now also includes 'renewable_percentage' and 'co2_intensity' if they were created\n",
        "    # dynamically add them to model_input_features\n",
        "    # Ensure this list is derived *after* the calculation steps in step 4.\n",
        "    final_model_input_features = [f for f in df_processed.select_dtypes(include=np.number).columns if f not in ['year', 'month', 'day', 'hour',\n",
        "                                                                        'dayofweek', 'dayofyear', 'quarter',\n",
        "                                                                        'month_sin', 'month_cos', 'hour_sin', 'hour_cos',\n",
        "                                                                        'dayofweek_sin', 'dayofweek_cos', 'fossilFuelPercentage',\n",
        "                                                                        'total_renewable', 'total_conventional', 'total_energy' # Exclude intermediate sums\n",
        "                                                                        ]]\n",
        "\n",
        "\n",
        "    wide_format = df_processed[final_model_input_features].copy()\n",
        "\n",
        "    # 6. Prepare Data for Specific Foundation Models (LagLlama Compatible Format)\n",
        "    print(\"\\nStep 6: Preparing LagLlama Compatible Format\")\n",
        "\n",
        "    # Frequency is now explicitly hourly ('H') as per meeting remarks (In [7])\n",
        "    most_common_freq = 'H'\n",
        "\n",
        "    # Create regular time index (already done by resample('H'))\n",
        "    # Reindex to regular frequency (interpolate missing values linearly)\n",
        "    lagllama_format = df_processed[final_model_input_features].copy() # This is the main DF for train/val/test splits\n",
        "\n",
        "    # 7. Split data for training/validation/testing\n",
        "    print(\"\\nStep 7: Creating Train/Val/Test Splits\")\n",
        "\n",
        "    total_len = len(lagllama_format)\n",
        "    # Keep 70% for train, 15% for validation, 15% for test\n",
        "    train_size = int(0.70 * total_len)\n",
        "    val_size = int(0.15 * total_len)\n",
        "\n",
        "    train_data = lagllama_format.iloc[:train_size]\n",
        "    val_data = lagllama_format.iloc[train_size : train_size + val_size]\n",
        "    test_data = lagllama_format.iloc[train_size + val_size :]\n",
        "\n",
        "    print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}, Test size: {len(test_data)}\")\n",
        "\n",
        "    # 8. Create metadata for foundation model\n",
        "    print(\"\\nStep 8: Creating Metadata for Foundation Model\")\n",
        "    metadata = {\n",
        "        'num_series': len(lagllama_format.columns), # Use columns from the final lagllama_format\n",
        "        'series_names': lagllama_format.columns.tolist(), # Ensure this is exactly what's in the CSV\n",
        "        'frequency': str(most_common_freq),\n",
        "        'start_date': str(df_processed.index.min()),\n",
        "        'end_date': str(df_processed.index.max()),\n",
        "        'train_start': str(train_data.index[0]),\n",
        "        'train_end': str(train_data.index[-1]),\n",
        "        'val_start': str(val_data.index[0]),\n",
        "        'val_end': str(val_data.index[-1]),\n",
        "        'test_start': str(test_data.index[0]),\n",
        "        'test_end': str(test_data.index[-1]),\n",
        "        # Ensure these lists only contain columns actually present in the final lagllama_format\n",
        "        'renewable_sources': [col for col in energy_sources if col in lagllama_format.columns],\n",
        "        'conventional_sources': [col for col in ['Braunkohle', 'Erdgas', 'Sonstige Konventionelle', 'Steinkohle'] if col in lagllama_format.columns],\n",
        "        'co2_column_name': 'carbonIntensity', # Explicitly use 'carbonIntensity'\n",
        "        'target_variables': [col for col in primary_target_columns if col in lagllama_format.columns] # Ensure targets are in final data\n",
        "    }\n",
        "    # Add prediction_length and context_length as per original LagLlama demos and user's processing notebook\n",
        "    metadata['prediction_length'] = 24\n",
        "    metadata['context_length'] = 504\n",
        "\n",
        "\n",
        "    return {\n",
        "        'processed_df': df_processed,\n",
        "        'wide_format': wide_format,\n",
        "        'lagllama_format': lagllama_format, # This is the main DF for train/val/test splits\n",
        "        'train_data': train_data,\n",
        "        'val_data': val_data,\n",
        "        'test_data': test_data,\n",
        "        'metadata': metadata\n",
        "    }"
      ],
      "metadata": {
        "id": "E30SzeA3aTw5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Execute Preprocessing ---\n",
        "processed_data = preprocess_energy_data_for_foundation_model(energy_df, price_df, co2_data_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o193Cfy8R0z6",
        "outputId": "b3765f90-5220-451a-9d18-cacb584fc6fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Data Cleaning and Preparation\n",
            "Data successfully resampled to hourly mean data.\n",
            "Data shape after cleaning, merging and hourly resampling: (4382, 13)\n",
            "Date range: 2024-12-10 15:00:00+00:00 to 2025-06-11 04:00:00+00:00\n",
            "Columns after selection for model and resampling: ['Erdgas', 'Sonstige Konventionelle', 'Biomasse', 'Braunkohle', 'Pumpspeicher', 'Steinkohle', 'Sonstige Erneuerbare', 'Photovoltaik', 'Wind Offshore', 'Wind Onshore', 'carbonIntensity', 'fossilFuelPercentage', 'Wasserkraft']\n",
            "\n",
            "Step 2: Handling Missing Values (post-resampling)\n",
            "Missing values - Before: 0, After: 0\n",
            "\n",
            "Step 3: Creating Time-based Features\n",
            "\n",
            "Step 4: Handling Renewable Percentage and CO2 Intensity\n",
            "Using 'fossilFuelPercentage' to derive 'renewable_percentage'.\n",
            "Using 'carbonIntensity' directly for 'co2_intensity' as per remarks.\n",
            "\n",
            "Step 5: Creating Foundation Model Ready Formats\n",
            "\n",
            "Step 6: Preparing LagLlama Compatible Format\n",
            "\n",
            "Step 7: Creating Train/Val/Test Splits\n",
            "Train size: 3067, Val size: 657, Test size: 658\n",
            "\n",
            "Step 8: Creating Metadata for Foundation Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save Processed Data to CSVs ---\n",
        "def save_processed_data(processed_data, output_dir='./processed_energy_data/'):\n",
        "    \"\"\"Save all processed datasets to CSV files\"\"\"\n",
        "    import os\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save main datasets\n",
        "    processed_data['processed_df'].to_csv(f'{output_dir}processed_energy_data.csv') # Index is now timestamp\n",
        "    processed_data['wide_format'].to_csv(f'{output_dir}wide_format.csv')\n",
        "    processed_data['lagllama_format'].to_csv(f'{output_dir}lagllama_format.csv')\n",
        "\n",
        "    # Save train/val/test splits\n",
        "    processed_data['train_data'].to_csv(f'{output_dir}train_data.csv')\n",
        "    processed_data['val_data'].to_csv(f'{output_dir}val_data.csv')\n",
        "    processed_data['test_data'].to_csv(f'{output_dir}test_data.csv')\n",
        "\n",
        "    # Save metadata\n",
        "    import json\n",
        "    with open(f'{output_dir}metadata.json', 'w') as f:\n",
        "        json.dump(processed_data['metadata'], f, indent=2)\n",
        "\n",
        "    print(f\"\\nAll processed data saved to {output_dir}\")\n",
        "    print(\"Generated files:\")\n",
        "    for file in sorted(os.listdir(output_dir)):\n",
        "        print(f\"  - {file}\")\n",
        "\n",
        "save_processed_data(processed_data)\n",
        "\n",
        "# --- Optional: Generate Profiling Report ---\n",
        "# df_for_profiling = processed_data['lagllama_format']\n",
        "# profile = ProfileReport(df_for_profiling, title=\"Processed Data Profiling Report\")\n",
        "# profile.to_file(\"processed_data_profiling_report.html\")\n",
        "# print(\"\\nProfiling report saved to processed_data_profiling_report.html\")\n",
        "\n",
        "# --- Quick Data Exploration Function ---\n",
        "def explore_processed_data(processed_data):\n",
        "    \"\"\"Quick exploration of the processed data\"\"\"\n",
        "\n",
        "    print(\"\\n=== PROCESSED DATA SUMMARY ===\")\n",
        "    print(f\"Original data shape (from InfluxDB fetch, then resampled hourly): {processed_data['processed_df'].shape}\")\n",
        "    print(f\"LagLlama format shape: {processed_data['lagllama_format'].shape}\")\n",
        "    print(f\"Time range: {processed_data['metadata']['start_date']} to {processed_data['metadata']['end_date']}\")\n",
        "    print(f\"Frequency: {processed_data['metadata']['frequency']}\")\n",
        "\n",
        "    print(\"\\n=== LAGLLAMA FORMAT DATA STATISTICS ===\")\n",
        "    stats_df = processed_data['lagllama_format']\n",
        "    print(stats_df.describe())\n",
        "\n",
        "    print(\"\\n=== MISSING VALUES CHECK (LagLlama Format) ===\")\n",
        "    missing_vals = processed_data['lagllama_format'].isnull().sum()\n",
        "    print(missing_vals[missing_vals > 0])\n",
        "\n",
        "    co2_col_name = processed_data['metadata']['co2_column_name']\n",
        "    if co2_col_name in stats_df.columns:\n",
        "        print(f\"\\n=== CORRELATION WITH {co2_col_name.upper()} ===\")\n",
        "        co2_corr = stats_df.corrwith(stats_df[co2_col_name])\n",
        "        print(co2_corr.sort_values(key=abs, ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24drKEATUT5p",
        "outputId": "1404906f-1e43-4bef-8103-fde3b6240c4d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All processed data saved to ./processed_energy_data/\n",
            "Generated files:\n",
            "  - lagllama_format.csv\n",
            "  - metadata.json\n",
            "  - processed_energy_data.csv\n",
            "  - test_data.csv\n",
            "  - train_data.csv\n",
            "  - val_data.csv\n",
            "  - wide_format.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_processed_data(processed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0g-lz5JUWDX",
        "outputId": "ed00f9c9-1f14-4629-c070-e3663995f6c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All processed data saved to ./processed_energy_data/\n",
            "Generated files:\n",
            "  - lagllama_format.csv\n",
            "  - metadata.json\n",
            "  - processed_energy_data.csv\n",
            "  - test_data.csv\n",
            "  - train_data.csv\n",
            "  - val_data.csv\n",
            "  - wide_format.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Quick Data Exploration Function ---\n",
        "def explore_processed_data(processed_data):\n",
        "    \"\"\"Quick exploration of the processed data\"\"\"\n",
        "\n",
        "    print(\"\\n=== PROCESSED DATA SUMMARY ===\")\n",
        "    print(f\"Original data shape (from InfluxDB fetch, then resampled hourly): {processed_data['processed_df'].shape}\")\n",
        "    print(f\"LagLlama format shape: {processed_data['lagllama_format'].shape}\")\n",
        "    print(f\"Time range: {processed_data['metadata']['start_date']} to {processed_data['metadata']['end_date']}\")\n",
        "    print(f\"Frequency: {processed_data['metadata']['frequency']}\")\n",
        "\n",
        "    print(\"\\n=== LAGLLAMA FORMAT DATA STATISTICS ===\")\n",
        "    stats_df = processed_data['lagllama_format']\n",
        "    print(stats_df.describe())\n",
        "\n",
        "    print(\"\\n=== MISSING VALUES CHECK (LagLlama Format) ===\")\n",
        "    missing_vals = processed_data['lagllama_format'].isnull().sum()\n",
        "    print(missing_vals[missing_vals > 0])\n",
        "\n",
        "    co2_col_name = processed_data['metadata']['co2_column_name']\n",
        "    if co2_col_name in stats_df.columns:\n",
        "        print(f\"\\n=== CORRELATION WITH {co2_col_name.upper()} ===\")\n",
        "        co2_corr = stats_df.corrwith(stats_df[co2_col_name])\n",
        "        print(co2_corr.sort_values(key=abs, ascending=False))"
      ],
      "metadata": {
        "id": "kmIlJYWIUai1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explore_processed_data(processed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pplw3vsSUf8u",
        "outputId": "4c91ab5c-4f40-43fe-a092-52ca097b8b53"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PROCESSED DATA SUMMARY ===\n",
            "Original data shape (from InfluxDB fetch, then resampled hourly): (4382, 28)\n",
            "LagLlama format shape: (4382, 14)\n",
            "Time range: 2024-12-10 05:00:00+00:00 to 2025-06-10 18:00:00+00:00\n",
            "Frequency: H\n",
            "\n",
            "=== LAGLLAMA FORMAT DATA STATISTICS ===\n",
            "       Pumpspeicher  Wind Onshore       Erdgas     Biomasse   Braunkohle  \\\n",
            "count   4382.000000   4382.000000  4382.000000  4382.000000  4382.000000   \n",
            "mean     315.943348   3059.025901  1911.341397  1050.053400  2047.987335   \n",
            "std      424.090081   2471.525370  1096.469193    77.714545   770.396582   \n",
            "min        0.000000     29.250000   384.750000   889.000000   486.000000   \n",
            "25%       20.750000   1131.187500  1029.500000   988.500000  1373.312500   \n",
            "50%       91.875000   2305.125000  1692.500000  1027.250000  2197.250000   \n",
            "75%      472.750000   4433.375000  2573.750000  1101.937500  2712.250000   \n",
            "max     1713.000000  11325.500000  5040.250000  1284.000000  3294.750000   \n",
            "\n",
            "       Wind Offshore  carbonIntensity  Photovoltaik  Sonstige Konventionelle  \\\n",
            "count    4382.000000      4382.000000   4382.000000              4382.000000   \n",
            "mean      676.346360       349.387190   1902.238761               417.115073   \n",
            "std       492.670236        98.996003   2994.786998                49.932231   \n",
            "min         0.000000       131.750000      0.000000               288.500000   \n",
            "25%       239.250000       270.000000      1.750000               385.750000   \n",
            "50%       571.750000       353.000000     13.750000               415.500000   \n",
            "75%      1091.437500       433.437500   2874.312500               447.000000   \n",
            "max      1984.250000       551.250000  12034.000000               681.000000   \n",
            "\n",
            "       Wasserkraft   Steinkohle  Sonstige Erneuerbare  renewable_percentage  \\\n",
            "count  4382.000000  4382.000000           4382.000000           4382.000000   \n",
            "mean    393.933592   962.409630             28.527841             60.436252   \n",
            "std      87.472412   566.504097              4.554944             13.156267   \n",
            "min     234.250000    58.750000             17.500000             32.337500   \n",
            "25%     334.500000   459.812500             24.250000             49.808125   \n",
            "50%     377.250000   982.625000             29.250000             60.352500   \n",
            "75%     437.000000  1460.250000             33.000000             71.105000   \n",
            "max     708.750000  2230.750000             36.500000             87.807500   \n",
            "\n",
            "       co2_intensity  \n",
            "count    4382.000000  \n",
            "mean      349.387190  \n",
            "std        98.996003  \n",
            "min       131.750000  \n",
            "25%       270.000000  \n",
            "50%       353.000000  \n",
            "75%       433.437500  \n",
            "max       551.250000  \n",
            "\n",
            "=== MISSING VALUES CHECK (LagLlama Format) ===\n",
            "Series([], dtype: int64)\n",
            "\n",
            "=== CORRELATION WITH CARBONINTENSITY ===\n",
            "co2_intensity              1.000000\n",
            "carbonIntensity            1.000000\n",
            "renewable_percentage      -0.994272\n",
            "Braunkohle                 0.857305\n",
            "Erdgas                     0.775002\n",
            "Steinkohle                 0.774890\n",
            "Photovoltaik              -0.600771\n",
            "Sonstige Erneuerbare       0.490234\n",
            "Biomasse                   0.468661\n",
            "Wind Onshore              -0.400531\n",
            "Sonstige Konventionelle    0.342328\n",
            "Pumpspeicher               0.199413\n",
            "Wind Offshore             -0.191747\n",
            "Wasserkraft                0.113007\n",
            "dtype: float64\n"
          ]
        }
      ]
    }
  ]
}